# -*- coding: utf-8 -*-
"""music3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HgEDZGv1Lup7FyXzZFejEwGtp6RzNfLm
"""

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

!pip install essentia

import numpy as np
import pandas as pd
import librosa
import os
from glob import glob
from scipy.stats import norm, kurtosis, skew, entropy
from tqdm import tqdm
from sklearn.model_selection import train_test_split, cross_val_score
import essentia
import essentia.standard as es

class deneme:
    a = pd.DataFrame()
    def fun(self):
        a['deneme'] = [1,2,3]
        self.a = a
        return self.a

class FeatureExtracter():
    df = pd.DataFrame()
    def __init__(self, path, mfcc=True, entropy=False):
        self.path = path
        self.data_files = glob(path + "/*/*.wav")
        self.spectral_features = ['spectral_centroid', 'spectral_bandwidth','spectral_contrast', 'spectral_flatness',
                    'spectral_rolloff', 'zero_crossing_rate','poly_features', 'rms']
        self.mfcc = mfcc
        self.entropy = entropy
        self.spectral_columns = []
        self.lpc_columns = []
        if mfcc:
            self.spectral_features.append('mfcc')
        
        self.genres = {v:k for k,v in enumerate([i[54:] for i in glob(path + "/*")])}
        self.fields = ['mean', 'std', 'kurtosis', 'skewness', 'median', 'min', 'max']
        if entropy:
            self.fields.append('entropy')
        
        classes = list(self.genres)

    
    def getColumnNames(self):
        features = []
        for i in self.spectral_features:
            features.append(i)
            features.append(i + "_delta1")
            features.append(i + "_delta2")

        spectral_columns = [i+"_"+j for i in features for j in self.fields]
        
        lpc_columns = ["LPC_"+i for i in self.fields]

        return spectral_columns, lpc_columns
    
    def calculate_values(self, arr):
        mean = arr.mean()
        std = arr.std()
        kurt = kurtosis(arr.T)[0]
        skewness = skew(arr.T)[0]
        median = np.median(arr)
        min_val = arr.min()
        max_val = arr.max()
        values = list((mean, std, kurt, skewness, median, min_val, max_val))
        if self.entropy:
            ent = entropy(arr)
            values.append(ent)
        return values

    def extract(self):
        spectral_df = []
        for i in tqdm(self.data_files):
            x, sr = librosa.load(i)
            line = []
            for k in self.spectral_features:
                if k == 'spectral_flatness':
                    arr = eval("librosa.feature.{}(x)".format(k))
                else:
                    arr = eval("librosa.feature.{}(x,sr)".format(k))
                arr_delta1 = librosa.feature.delta(arr, order=1)
                arr_delta2 = librosa.feature.delta(arr, order=2)
                line += self.calculate_values(arr)
                line += self.calculate_values(arr_delta1)
                line += self.calculate_values(arr_delta2)
            spectral_df.append(line.copy())

        lpc_df = []
        for i in tqdm(self.data_files):
            line = []
            audio = es.MonoLoader(filename=i)()
            lpc = es.LPC()(audio)[0]
            mean = lpc.mean()
            std = lpc.std()
            kurt = kurtosis(lpc)
            skewness = skew(lpc)
            median = np.median(lpc)
            min_val = lpc.min()
            max_val = lpc.max()
            line += [mean, std, kurt, skewness, median, min_val, max_val]
            lpc_df.append(line)
        
        spectral_columns, lpc_columns = self.getColumnNames()

        spectral_df = pd.DataFrame(data=spectral_df, columns=spectral_columns)
        lpc_df = pd.DataFrame(data=lpc_df, columns=lpc_columns)
        df = pd.concat([spectral_df, lpc_df], axis=1)
        labels = [self.genres[k] for k in [i[54:54+(i[54:].find("/"))] for i in self.data_files]]
        self.df = df
        return self.df

    def save(self, path):
        self.df.to_pickle(path, protocol=4)

extracter = FeatureExtracter("/content/drive/MyDrive/For_Colab/Data/genres_original",
                             mfcc=True, entropy=False)

df = extracter.extract()

extracter.save("/content/drive/MyDrive/For_Colab/Data/extracter.pkl")

class ModelSelection():
    def __init__(df, k, random_forest=True, logistic_reg=True, adaboost=True, decision_tree=True,
                 knn=True, svm=True, naive_bayes=True, mlp=True):

        self.df = df
        self.k = k
        self.random_forest = random_forest
        self.logistic_reg = logistic_reg
        self.adaboost = adaboost
        self.decision_tree = decision_tree
        self.knn = knn
        self.svm = svm
        self.naive_bayes = naive_bayes
        self.mlp = mlp

    def apply_methods(self):
        rf1 = RandomForestClassifier(n_estimators=100, max_depth=10, max_features=0.6, min_samples_split=2)
        rf2 = RandomForestClassifier(n_estimators=100, max_depth=20, max_features=0.6, min_samples_split=2)
        rf3 = RandomForestClassifier(n_estimators=100, max_depth=10, max_features='sqrt', min_samples_split=2)
        rf4 = RandomForestClassifier(n_estimators=100, max_depth=20, max_features='sqrt', min_samples_split=2)
        rf5 = RandomForestClassifier(n_estimators=100, max_depth=50, max_features='auto', min_samples_split=2)
        lgr1 = LogisticRegression(C=10, max_iter=500, n_jobs=-1)
        lgr2 = LogisticRegression(C=1, max_iter=500, n_jobs=-1)
        lgr3 = LogisticRegression(C=0.1, max_iter=500, n_jobs=-1)
        lgr4 = LogisticRegression(C=0.01, max_iter=500, n_jobs=-1)
        lgr5 = LogisticRegression(C=0.001, max_iter=500, n_jobs=-1)
        ada1 = AdaBoostClassifier(n_estimators=100, learning_rate=1, algorithm='SAMME.R')
        ada2 = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, algorithm='SAMME.R')
        ada3 = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, algorithm='SAMME.R')
        ada4 = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, algorithm='SAMME')
        ada5 = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, algorithm='SAMME')
        dt1 = DecisionTreeClassifier(criterion='gini', max_depth=10)
        dt2 = DecisionTreeClassifier(criterion='gini', max_depth=30)
        dt3 = DecisionTreeClassifier(criterion='entropy', max_depth=10)
        dt4 = DecisionTreeClassifier(criterion='entropy', max_depth=30)
        dt5 = DecisionTreeClassifier(criterion='gini', max_depth=50)
        knn1 = KNeighborsClassifier(n_neighbors=1, weights='uniform')
        knn2 = KNeighborsClassifier(n_neighbors=1, weights='distance')
        knn3 = KNeighborsClassifier(n_neighbors=2, weights='distance')
        knn4 = KNeighborsClassifier(n_neighbors=3, weights='uniform')
        knn5 = KNeighborsClassifier(n_neighbors=3, weights='distance')
        svm1 = SVC(kernel='linear')
        svm2 = SVC(kernel='poly', degree=3)
        svm3 = SVC(kernel='poly', degree=4)
        svm4 = SVC(kernel='poly', degree=5)
        svm5 = SVC(kernel='poly', degree=5, C=0.1)

